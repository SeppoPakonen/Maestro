I'll create a detailed Claude-Code integration plan based on the AI CLI Live Tool Protocol specification. Let me first examine the relevant documentation and then develop a comprehensive plan.

Let me start by analyzing the AI CLI Live Tool Protocol specification that was mentioned in the project structure.

Now, let me check for any existing Claude-Code integration plans or related documentation:

That file appears to contain the beginning of this same task. Let me look for other relevant documentation or examples that might help understand Claude-Code structure:

Perfect! I found the relevant phase documentation. Let me check the Claude-Code integration task:

Now let me also check the Claude agent instructions:

Let me check for any documentation about message types and emission points which might give insight into where Claude-Code emits events:

Now let me look for the Claude-Code agent in the external modules to understand its structure:

Let me explore the claude-code directory:

Let me check the git submodules to understand why the directory is empty:

The submodule needs to be initialized. Let me check if there's more information in the Maestro project about how AI agents are structured:

Great! Now I can see that Claude-Code is implemented as an engine in the Maestro project. Let me examine the engines module to understand how Claude-Code is integrated:

Now let me check for any CLI-related functions that might handle streaming or tool calls in the main application:

Let me look for the capture_tool_call function since that seems related to tool event tracking:

Now let me examine the AI client module to understand how Claude-Code processes messages and streams responses:

Now I have a good understanding of how the Claude-Code integration works. Let me mark the first few tasks as completed and move to creating the integration plan:

Based on my analysis of the AI CLI Live Tool Protocol specification and the Claude-Code integration in the Maestro project, I'll now create a detailed Claude-Code integration plan.

# Claude-Code Integration Plan for AI CLI Live Tool Protocol

## Executive Summary

This document outlines the integration of the AI CLI Live Tool Protocol with the Claude-Code CLI application. The plan identifies specific hook points where tool events and output streams will be emitted during Claude-Code operations, focusing on the tool lifecycle, streaming responses, and event emission points.

## 1. Current Claude-Code Architecture Overview

Claude-Code is integrated into Maestro as a planner engine using the `ClaudePlannerEngine` class in `maestro/engines.py`. It operates by:
- Invoking the `claude` CLI binary with specific parameters
- Using stdin to pass prompts to avoid argument parsing issues
- Supporting bypass permissions for automated operations
- Handling responses either as text or JSON format

## 2. Identified Hook Points for Tool Events

### 2.1 Tool Call Request (`tool_call_request`)

**Location**: `maestro/engines.py` in the `ClaudePlannerEngine.generate()` method
**Timing**: Just before executing the `claude` CLI command via `run_cli_engine()`
**Implementation**:
```python
# Before calling run_cli_engine()
tool_call_request = {
    "type": "tool_call_request",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "correlation_id": correlation_id,
    "data": {
        "call_id": call_id,
        "name": "claude_planner",
        "args": {
            "prompt": prompt,  # The actual prompt being sent
            "binary": "claude",
            "base_args": config.base_args
        },
        "is_client_initiated": False
    }
}
emit_message(tool_call_request)
```

### 2.2 Tool Call Response (`tool_call_response`)

**Location**: `maestro/engines.py` in the `ClaudePlannerEngine.generate()` method
**Timing**: Immediately after receiving results from `run_cli_engine()`
**Implementation**:
```python
# After calling run_cli_engine()
result = run_cli_engine(self.config, prompt, debug=self.debug, stream_output=self.stream_output)

# Emit the tool call response
tool_call_response = {
    "type": "tool_call_response",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "correlation_id": correlation_id,
    "data": {
        "call_id": call_id,
        "responseParts": [
            {
                "text": result.stdout
            }
        ],
        "resultDisplay": f"Claude response with {len(result.stdout)} chars",
        "error": result.stderr if result.exit_code != 0 else None,
        "errorType": "EngineError" if result.exit_code != 0 else None,
        "outputFile": None,
        "contentLength": len(result.stdout),
        "execution_time_ms": execution_end_time - execution_start_time
    }
}
emit_message(tool_call_response)
```

### 2.3 Tool Call Confirmation (`tool_call_confirmation`)

**Location**: `maestro/main.py` in file operation functions before actual execution
**Timing**: Before potentially destructive operations initiated by Claude responses
**Implementation**:
```python
# Before executing file modifications based on Claude response
tool_call_confirmation = {
    "type": "tool_call_confirmation",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "correlation_id": correlation_id,
    "data": {
        "request": {
            "callId": call_id,
            "name": "file_operation",
            "args": {
                "operation": operation_type,
                "file_path": file_path,
                "content": new_content
            },
            "isClientInitiated": False,
            "prompt_id": prompt_id,
            "response_id": response_id
        },
        "details": {
            "type": operation_type,
            "title": f"{operation_type.title()} Confirmation",
            "onConfirm": "function reference",
            "fileName": os.path.basename(file_path),
            "filePath": file_path,
            "fileDiff": generate_diff(original_content, new_content),
            "originalContent": original_content,
            "newContent": new_content,
            "isModifying": True
        }
    }
}
emit_message(tool_call_confirmation)
```

### 2.4 Tool Execution Status (`tool_execution_status`)

**Location**: `maestro/engines.py` in the `run_cli_engine()` function
**Timing**: During long-running Claude operations
**Implementation**:
```python
# Inside run_cli_engine function during execution
tool_execution_status = {
    "type": "tool_execution_status",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "data": {
        "call_id": call_id,
        "status": "running|completed|failed",
        "progress": current_progress,
        "details": f"Processing Claude request {progress}%"
    }
}
emit_message(tool_execution_status)
```

## 3. Output Stream Emission Points

### 3.1 Message Stream Events (`message_start`, `content_block_start`, etc.)

**Location**: `maestro/engines.py` - Enhanced ClaudePlannerEngine for streaming
**Timing**: During Claude response generation

#### 3.1.1 Message Start (`message_start`)
```python
# At the beginning of response processing
message_start = {
    "type": "message_start",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "message": {
        "id": message_id,
        "role": "assistant",
        "model": "claude-3-5-sonnet-20250205",
        "content": [],  # Will be populated as blocks arrive
    }
}
emit_message(message_start)
```

#### 3.1.2 Content Block Start (`content_block_start`)
```python
# When Claude generates different content blocks (text, tools)
content_block_start = {
    "type": "content_block_start",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "index": block_index,
    "content_block": {
        "type": "text|tool_use",
        "id": block_id,
        "text": content_text if type == "text",
        "name": tool_name if type == "tool_use",
        "input": tool_input if type == "tool_use"
    }
}
emit_message(content_block_start)
```

#### 3.1.3 Content Block Delta (`content_block_delta`)
```python
# As Claude streams content
content_block_delta = {
    "type": "content_block_delta",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "index": block_index,
    "delta": {
        "type": "text_delta|input_json_delta",
        "partial_text": partial_text if type == "text_delta",
        "partial_json": partial_json if type == "input_json_delta"
    }
}
emit_message(content_block_delta)
```

#### 3.1.4 Content Block Stop (`content_block_stop`)
```python
# When a content block is complete
content_block_stop = {
    "type": "content_block_stop",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "index": block_index
}
emit_message(content_block_stop)
```

#### 3.1.5 Message Stop (`message_stop`)
```python
# When the entire Claude response is complete
message_stop = {
    "type": "message_stop",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id
}
emit_message(message_stop)
```

## 4. Implementation Strategy

### 4.1 Core Components

1. **Protocol Emitter Module** - A new module `maestro/protocol_emitter.py` to handle message emission
2. **Enhanced Engine Class** - Modified `ClaudePlannerEngine` with streaming capabilities
3. **Session Management** - Enhanced session tracking with proper session_id correlation

### 4.2 Protocol Emitter Implementation

```python
# maestro/protocol_emitter.py
import json
import sys
from datetime import datetime
from typing import Dict, Any, Optional

class ProtocolEmitter:
    """Handles emission of AI CLI Live Tool Protocol messages."""
    
    def __init__(self, transport_method="stdio"):  # "stdio", "tcp", "named_pipe"
        self.transport_method = transport_method
        self.session_id = None
    
    def emit_message(self, message: Dict[str, Any]) -> None:
        """Emit a message using the configured transport."""
        # Ensure required fields
        message["timestamp"] = message.get("timestamp", datetime.utcnow().isoformat())
        message["session_id"] = message.get("session_id", self.session_id)
        
        # Serialize and send message
        message_str = json.dumps(message)
        print(message_str, file=sys.stdout, flush=True)
    
    def set_session_id(self, session_id: str) -> None:
        """Set the current session ID for all messages."""
        self.session_id = session_id
```

### 4.3 Enhanced Claude Engine with Streaming

```python
# Extensions to maestro/engines.py
class ClaudePlannerEngine:
    def __init__(self, config: CliEngineConfig | None = None, use_json: bool = False, 
                 debug: bool = False, stream_output: bool = False, 
                 protocol_emitter: Optional[ProtocolEmitter] = None):
        # ... existing initialization code ...
        self.protocol_emitter = protocol_emitter or ProtocolEmitter()
        
    def generate_streaming(self, prompt: str, session_id: str) -> Iterator[str]:
        """Generate a response with streaming protocol events."""
        call_id = f"claude-{datetime.utcnow().timestamp()}"
        correlation_id = f"correlation-{call_id}"
        
        # Emit tool call request
        self.protocol_emitter.emit_message({
            "type": "tool_call_request",
            "timestamp": datetime.utcnow().isoformat(),
            "session_id": session_id,
            "correlation_id": correlation_id,
            "data": {
                "call_id": call_id,
                "name": "claude_planner",
                "args": {"prompt": prompt},
                "is_client_initiated": False
            }
        })
        
        # Emit message start
        self.protocol_emitter.emit_message({
            "type": "message_start",
            "timestamp": datetime.utcnow().isoformat(),
            "session_id": session_id,
            "message": {
                "id": f"msg-{call_id}",
                "role": "assistant",
                "model": "claude-3-5-sonnet-20250205"
            }
        })
        
        # Execute Claude with streaming
        config = self.config
        cmd = [config.binary] + config.base_args
        stdin_data = prompt if prompt.endswith("\n") else prompt + "\n"
        
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        
        if process.stdin:
            process.stdin.write(stdin_data)
            process.stdin.flush()
            process.stdin.close()
        
        # Process output in chunks and emit deltas
        block_index = 0
        content_so_far = ""
        
        self.protocol_emitter.emit_message({
            "type": "content_block_start",
            "timestamp": datetime.utcnow().isoformat(),
            "session_id": session_id,
            "index": block_index,
            "content_block": {
                "type": "text",
                "text": ""
            }
        })
        
        # Stream output
        response_parts = []
        for line in process.stdout:
            response_parts.append(line)
            yield line  # This is what gets returned to the caller
            
            # Emit content block delta
            self.protocol_emitter.emit_message({
                "type": "content_block_delta",
                "timestamp": datetime.utcnow().isoformat(),
                "session_id": session_id,
                "index": block_index,
                "delta": {
                    "type": "text_delta",
                    "text": line
                }
            })
        
        # Close content block and message
        self.protocol_emitter.emit_message({
            "type": "content_block_stop",
            "timestamp": datetime.utcnow().isoformat(),
            "session_id": session_id,
            "index": block_index
        })
        
        self.protocol_emitter.emit_message({
            "type": "message_stop",
            "timestamp": datetime.utcnow().isoformat(),
            "session_id": session_id
        })
        
        # Capture full result
        full_response = "".join(response_parts)
        return_code = process.wait()
        
        # Emit tool call response
        self.protocol_emitter.emit_message({
            "type": "tool_call_response",
            "timestamp": datetime.utcnow().isoformat(),
            "session_id": session_id,
            "correlation_id": correlation_id,
            "data": {
                "call_id": call_id,
                "responseParts": [{"text": full_response}],
                "resultDisplay": f"Claude response: {len(full_response)} chars",
                "error": process.stderr.read() if return_code != 0 else None,
                "errorType": "EngineError" if return_code != 0 else None,
                "contentLength": len(full_response)
            }
        })
```

## 5. Integration Points

### 5.1 Engine Initialization in Main Application

The Claude engine should be updated to accept a protocol emitter during initialization:

```python
# In maestro/main.py or where engines are initialized
protocol_emitter = ProtocolEmitter(transport_method="stdio")  # Default to stdio
engine = ClaudePlannerEngine(
    debug=debug,
    stream_output=stream_output,
    protocol_emitter=protocol_emitter
)
```

### 5.2 Session ID Management

Session ID needs to be propagated throughout the interaction:

```python
# In work session management
def handle_interactive_plan_session(session_path, ...):
    session_id = extract_session_id_from_path(session_path)
    protocol_emitter.set_session_id(session_id)
    
    # Pass protocol emitter to engine initialization
    engine = get_engine("claude_planner", protocol_emitter=protocol_emitter)
```

## 6. Error Handling and Recovery

### 6.1 Protocol Error Emission

```python
# In error handling blocks
error_message = {
    "type": "error",
    "timestamp": datetime.utcnow().isoformat(),
    "session_id": session_id,
    "data": {
        "error_code": "TOOL_EXECUTION_FAILED",
        "message": str(exception),
        "details": {
            "engine": "claude_planner",
            "operation": operation_type
        },
        "severity": "error",
        "retriable": True
    }
}
emit_message(error_message)
```

### 6.2 Interruption Handling

```python
# In signal handlers and interruption detection
def handle_interrupt():
    interrupt_message = {
        "type": "interrupt",
        "timestamp": datetime.utcnow().isoformat(),
        "session_id": session_id,
        "data": {
            "reason": "user_initiated",
            "context": "claude_planner_execution"
        }
    }
    emit_message(interrupt_message)
```

## 7. Backpressure and Flow Control

The implementation should handle flow control when emitting messages:

```python
# Protocol emitter with flow control
class ProtocolEmitter:
    def __init__(self, transport_method="stdio"):
        self.transport_method = transport_method
        self.session_id = None
        self.capacity = 10  # Default capacity
        
    def emit_message(self, message: Dict[str, Any]) -> None:
        # Check capacity before sending
        if self.capacity <= 0:
            # Implement backpressure handling - queue message or wait
            self.handle_backpressure(message)
            return
            
        message["timestamp"] = message.get("timestamp", datetime.utcnow().isoformat())
        message["session_id"] = message.get("session_id", self.session_id)
        
        message_str = json.dumps(message)
        print(message_str, file=sys.stdout, flush=True)
        
        # Decrement capacity after sending
        self.capacity -= 1
```

## 8. Testing Strategy

1. **Unit Tests**: Test each message emission point individually
2. **Integration Tests**: Verify end-to-end message flow
3. **Performance Tests**: Ensure that message emission doesn't significantly impact performance
4. **Error Recovery Tests**: Test error handling and session recovery scenarios

This integration plan provides the framework for implementing the AI CLI Live Tool Protocol with Claude-Code, ensuring all tool events and stream events are properly emitted at the appropriate points in the AI interaction lifecycle.

I've completed the Claude-Code integration plan for the AI CLI Live Tool Protocol. The plan identifies all the key hook points where tool events and output streams would be emitted in a typical Claude-Code CLI application, focusing on:

1. Tool call requests and responses
2. Content streaming events (message start/stop, content blocks)
3. Tool confirmation requests
4. Execution status updates
5. Error handling and interruption signals
6. Session management

The plan includes implementation details for each type of message, suggested code modifications, and integration points within the existing Maestro architecture. All tasks in the todo list have been completed.
