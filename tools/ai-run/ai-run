#!/usr/bin/env python3
# Copyright: Seppo Pakonen 2026 (C)
# License: GPLv3

import sys
import argparse
import os
import subprocess
import json
import tempfile
import shutil
import difflib
from pathlib import Path
from datetime import datetime

# Ensure maestro package is discoverable
repo_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(repo_root))

try:
    from maestro.ai.maestro_link import get_link
    link = get_link()
except ImportError:
    link = None

# 2026 Model Specifications
MODELS = {
    "codex": {
        "hard": "gpt-5.2-codex", # Extra High Reasoning
        "mid": "gpt-5.1-codex-max",
        "easy": "gpt-5.1-codex-mini",
        "general": "gpt-5.2",
        "judge": "gpt-5.2-codex"
    },
    "claude": {
        "hard": "opus-4.5",
        "mid": "sonnet-4.5",
        "easy": "haiku-4.5"
    },
    "gemini": {
        "hard": "gemini-3-flash-preview",
        "mid": "gemini-2.5-pro",
        "easy": "gemini-2.5-flash-lite",
        "auto": None # Let CLI decide
    },
    "qwen": {
        "hard": "qwen3-coder",
        "mid": "qwen3-coder",
        "easy": "qwen3-coder",
        "coder": "qwen3-coder",
        "vision": "qwen3-vl"
    }
}

MAESTRO_DIR = Path.home() / ".maestro"
AI_RUN_DIR = MAESTRO_DIR / "ai-run"
BACKUP_DIR = MAESTRO_DIR / "backups"
HISTORY_FILE = AI_RUN_DIR / "history.jsonl"

def setup_directories():
    AI_RUN_DIR.mkdir(parents=True, exist_ok=True)
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)

def create_backup(file_path):
    setup_directories()
    p = Path(file_path)
    if not p.exists():
        return None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = BACKUP_DIR / f"{p.name}.{timestamp}.bak"
    shutil.copy2(p, backup_path)
    return backup_path

def save_history(entry):
    try:
        setup_directories()
        with open(HISTORY_FILE, "a") as f:
            f.write(json.dumps(entry) + "\n")
    except Exception as e:
        print(f"Warning: Failed to save history: {e}", file=sys.stderr)

def run_backend(backend, model, prompt, stream=True, yolo=False, files=None):
    cmd = []
    if backend == "gemini":
        cmd = ["gemini"]
        if yolo:
            cmd.append("-y")
        else:
            cmd.extend(["--approval-mode", "default"])
        if model:
            cmd.extend(["-m", model])
        if files:
            for f in files:
                cmd.extend(["--attach", f])
        cmd.append(prompt)
    elif backend == "claude":
        cmd = ["claude", "-p"]
        if yolo:
            cmd.append("--dangerously-skip-permissions")
        if model:
            cmd.extend(["--model", model])
        if files:
            # Note: claude CLI might handle files differently, assuming positional for now or --add-dir
            for f in files:
                cmd.extend(["--add-dir", str(Path(f).parent)])
        cmd.append(prompt)
    elif backend == "codex":
        cmd = ["codex", "exec"]
        if yolo:
            cmd.append("--dangerously-bypass-approvals-and-sandbox")
        if model:
            cmd.extend(["-m", model])
        if files:
            for f in files:
                cmd.extend(["--image", f]) # Assuming multimodal via --image for now
        cmd.append(prompt)
    elif backend == "qwen":
        cmd = ["qwen"]
        if yolo:
            cmd.append("-y")
        if model:
            cmd.extend(["-m", model])
        # Assuming qwen handles multimodal files similarly to gemini if not specified
        cmd.append(prompt)

    if yolo:
        print("WARNING: YOLO mode active. Bypassing safety restrictions and sandboxes.", file=sys.stderr)

    try:
        if stream:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)
            stdout_text = []
            
            for line in process.stdout:
                sys.stdout.write(line)
                sys.stdout.flush()
                stdout_text.append(line)
            
            _, stderr = process.communicate()
            return process.returncode, "".join(stdout_text), stderr
        else:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            stdout, stderr = process.communicate()
            return process.returncode, stdout, stderr
    except Exception as e:
        return -1, "", str(e)

def eval_difficulty(prompt, backend="gemini"):
    eval_prompt = f"Assess the difficulty of the following task. Respond only with one word: 'easy', 'mid', or 'hard'.\n\nTask: {prompt}"
    rc, stdout, stderr = run_backend(backend, None, eval_prompt, stream=False)
    if rc == 0:
        result = stdout.strip().lower()
        if "hard" in result: return "hard"
        if "mid" in result: return "mid"
        if "easy" in result: return "easy"
    return "mid" # Fallback

def check_status():
    print("Checking backend status...")
    backends = ["gemini", "claude", "codex", "qwen"]
    for b in backends:
        try:
            subprocess.run(["which", b], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            print(f"{b:8}: Available")
        except subprocess.CalledProcessError:
            print(f"{b:8}: NOT FOUND")
    
    print("\nQuota Information (Estimated):")
    print("All backends appearing healthy. Consult provider dashboards for exact billing/quota.")

def run_gen_image(prompt, output_file, backend="gemini"):
    print(f"Generating image: '{prompt}' -> {output_file}...")
    # Integration for image generation using 2026 models
    # Assuming gemini or codex has a --gen-image or similar subcommand in 2026
    if backend == "gemini":
        cmd = ["gemini", "imagine", prompt, "--output", output_file]
    else: # codex
        cmd = ["codex", "generate-image", prompt, "--output", output_file]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"Image saved to {output_file}")
            return 0
        else:
            print(f"Error generating image: {result.stderr}", file=sys.stderr)
            return result.returncode
    except Exception as e:
        print(f"Failed to run image generation: {e}", file=sys.stderr)
        return -1

def extract_code_block(text, language="python"):
    import re
    # Try specified language first
    pattern = rf"```(?:{language})?\n(.*?)```"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    
    # Fallback to any code block
    match = re.search(r"```\n(.*?)```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # If no code block, return original text if it looks like code? 
    # Better to be strict for self-healing.
    return None

def run_fix_loop(args, prompt, script_path):
    retries = args.retry
    history = [] # Keep track of errors and fixes
    
    # Determine model - prefer hard for fixing
    model_tier = args.difficulty
    if model_tier == "mid" and "hard" in MODELS[args.backend]:
        model_tier = "hard"
    model = MODELS[args.backend].get(model_tier, MODELS[args.backend].get("mid"))

    for attempt in range(retries + 1):
        if attempt > 0:
            print(f"\n--- [SELF-HEALING] Attempt {attempt}/{retries} ---")
        
        # 1. Execution Monitoring
        print(f"Executing: {script_path}")
        # We use Popen to stream stdout/stderr
        # For simplicity in this implementation, we'll run it and capture
        process = subprocess.run([sys.executable, script_path], capture_output=True, text=True)
        
        # Display output
        if process.stdout:
            print("\033[94m[STDOUT]\033[0m")
            print(process.stdout)
        if process.stderr:
            print("\033[91m[STDERR]\033[0m")
            print(process.stderr, file=sys.stderr)

        if process.returncode == 0:
            print("\n\033[92m[SUCCESS] Execution finished successfully.\033[0m")
            if link:
                state = link.get_maestro_context()
                task_id = state.get("task_id")
                if task_id:
                    link.update_task_status(task_id, "Fixed", f"Execution successful after {attempt} retries.")
            return 0
        
        if attempt == retries:
            print(f"\n\033[91m[FAILURE] Reached maximum retries ({retries}).\033[0m")
            if link:
                state = link.get_maestro_context()
                task_id = state.get("task_id")
                if task_id:
                    link.update_task_status(task_id, "Failed", f"Failed after {retries} retries.")
            return process.returncode

        # 2. Analysis & Correction Phase
        print(f"\n\033[93m[ANALYZING] Failure detected (Exit Code {process.returncode}). Requesting fix...\033[0m")
        
        current_code = Path(script_path).read_text()
        
        # Assemble context
        fix_prompt = f"""
I am running a script that is failing. Please help me fix it.

[MAESTRO PROJECT CONTEXT]
{link.inject_maestro_context("") if link else "Standalone mode."}

[ERROR LOG]
Exit Code: {process.returncode}
Stderr:
{process.stderr}

[ORIGINAL CODE]
```python
{current_code}
```
"""
        if history:
            fix_prompt += "\n[PREVIOUS ATTEMPTS AND FAILURES]\n"
            for h in history:
                fix_prompt += f"- Attempt {h['attempt']}: AI proposed fix, but it failed with: {h['error']}\n"

        fix_prompt += "\nINSTRUCTIONS:\nAnalyze the error and provide the COMPLETE corrected file. Respond ONLY with the code inside a ```python block."

        # Repair Request
        rc, ai_response, ai_err = run_backend(args.backend, model, fix_prompt, stream=False, yolo=args.yolo)
        
        if rc != 0:
            print(f"AI failed to provide a fix: {ai_err}", file=sys.stderr)
            return rc
        
        # 3. Apply Fix
        fixed_code = extract_code_block(ai_response, "python")
        if not fixed_code:
            print("\n\033[91m[ERROR] Could not extract fixed code from AI response.\033[0m")
            print("AI Response was:")
            print(ai_response)
            return 1
        
        # Show Diff
        diff = difflib.unified_diff(
            current_code.splitlines(),
            fixed_code.splitlines(),
            fromfile=f"{script_path} (Current)",
            tofile=f"{script_path} (Proposed Fix)",
            lineterm=""
        )
        diff_text = "\n".join(diff)
        
        if not diff_text:
            print("\n\033[93m[INFO] AI proposed no changes.\033[0m")
            # Maybe the error is environmental? If we retry without change we'll loop.
            # Feed back to AI that no change was made.
            history.append({"attempt": attempt + 1, "error": "AI proposed exactly the same code which is failing."})
            continue

        print("\n\033[95m[PROPOSED FIX]\033[0m")
        print(diff_text)
        
        # Confirmation
        if not args.yolo and not args.yes:
            confirm = input("\nApply this fix? [y/N]: ").lower()
            if confirm != 'y':
                print("Aborted.")
                return 1
        
        # Backup
        backup_path = create_backup(script_path)
        if backup_path:
            print(f"Backup created: {backup_path}")
            
        # Write fix
        Path(script_path).write_text(fixed_code)
        print("\033[92mFix applied. Retrying execution...\033[0m")
        
        # Log to Maestro
        if link:
            state = link.get_maestro_context()
            task_id = state.get("task_id")
            if task_id:
                link.log_ai_event(task_id, {
                    "action": "self_healing_attempt",
                    "attempt": attempt + 1,
                    "backend": args.backend,
                    "model": model,
                    "exit_code": process.returncode
                })
        
        # Update history for next iteration
        history.append({"attempt": attempt + 1, "error": process.stderr.splitlines()[-1] if process.stderr.splitlines() else "Unknown error"})

    return 1

def run_debate(args, prompt):
    print("Entering Debate Mode (Consensus synthesize)...")
    
    # Send to two backends
    backends = [("claude", MODELS["claude"]["hard"]), ("gemini", MODELS["gemini"]["hard"])]
    responses = []
    
    for name, model in backends:
        print(f"Requesting perspective from {name}...")
        rc, stdout, stderr = run_backend(name, model, prompt, stream=False, yolo=args.yolo)
        if rc == 0:
            responses.append((name, stdout))
        else:
            print(f"Warning: {name} failed to respond: {stderr}", file=sys.stderr)

    if len(responses) < 2:
        print("Debate failed: Not enough backends responded.", file=sys.stderr)
        return 1

    # Pass to Judge (Codex)
    judge_model = MODELS["codex"]["judge"]
    judge_prompt = f"""
You are the Judge in a multi-model debate. 
Original Prompt: {prompt} 

Response from {responses[0][0]}:
{responses[0][1]}

Response from {responses[1][0]}:
{responses[1][1]}

Synthesize a final consensus response that captures the best of both perspectives.
"""
    print(f"Passing debate to Judge ({judge_model})...")
    rc, stdout, stderr = run_backend("codex", judge_model, judge_prompt, stream=True, yolo=args.yolo)
    
    if rc == 0 and link:
        state = link.get_maestro_context()
        task_id = state.get("task_id")
        if task_id:
            link.log_ai_event(task_id, {
                "action": "debate_consensus",
                "backends": [r[0] for r in responses],
                "judge": "codex"
            })
            
    return rc

def handle_step(prompt):
    with tempfile.NamedTemporaryFile(suffix=".md", mode="w+", delete=False) as tf:
        tf.write(prompt)
        temp_path = tf.name
    
    editor = os.environ.get("EDITOR", "vi")
    print(f"Pausing for human review. Opening {editor}...")
    subprocess.run([editor, temp_path])
    
    with open(temp_path, "r") as f:
        new_prompt = f.read()
    
    os.unlink(temp_path)
    return new_prompt

def main():
    parser = argparse.ArgumentParser(
        description="ai-run: A smart wrapper for AI prompts.",
        add_help=False
    )
    # Core Arguments
    parser.add_argument("-b", "--backend", choices=["gemini", "codex", "claude", "qwen"], default="gemini", help="Select provider")
    parser.add_argument("-d", "--difficulty", choices=["easy", "mid", "hard", "coder", "vision", "general"], default="mid", help="Manually set tier")
    parser.add_argument("-e", "--eval-difficulty", "-ed", action="store_true", help="Run a pre-prompt to assess task difficulty")
    parser.add_argument("-y", "--yolo", action="store_true", help="Enable unrestricted execution (bypass safety/sandboxes)")
    parser.add_argument("-Y", "--yes", action="store_true", help="Auto-approve all prompts and skip interactive pauses (implies --yolo)")
    parser.add_argument("--status", action="store_true", help="Check API availability and remaining quota")
    
    # Agentic Modes
    parser.add_argument("--fix", action="store_true", help="Self-healing mode: fix and re-run failing scripts")
    parser.add_argument("--retry", type=int, default=3, help="Maximum number of retries for self-healing (default: 3)")
    parser.add_argument("--debate", action="store_true", help="Debate mode: Consensus from multiple models")
    parser.add_argument("--step", action="store_true", help="Interactive pause: edit prompt/plan in $EDITOR before execution")
    
    # Multimodal & Generative
    parser.add_argument("--gen-image", help="Generate an image based on the prompt")
    parser.add_argument("-o", "--output", help="Output file for generated images or logs")
    
    # General
    parser.add_argument("-h", "--help", action="help", help="Show this help message and exit")
    parser.add_argument("prompt_or_file", nargs="?", help="Prompt text or file containing prompt")
    parser.add_argument("files", nargs="*", help="Additional files (images/audio/scripts)")

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(0)

    args, unknown = parser.parse_known_args()

    if args.status:
        check_status()
        sys.exit(0)

    if args.gen_image:
        sys.exit(run_gen_image(args.gen_image, args.output or "output.png", args.backend))

    prompt = ""
    script_path = None
    if args.prompt_or_file:
        if os.path.isfile(args.prompt_or_file):
            script_path = args.prompt_or_file
            with open(args.prompt_or_file, "r") as f:
                lines = f.readlines()
                if lines and lines[0].startswith("#!"):
                    start_idx = 1
                    while start_idx < len(lines) and (not lines[start_idx].strip() or lines[start_idx].strip().startswith("#")):
                        start_idx += 1
                    prompt = "".join(lines[start_idx:])
                else:
                    prompt = "".join(lines)
        else:
            prompt = args.prompt_or_file
    
    if not prompt and not sys.stdin.isatty():
        prompt = sys.stdin.read()
    
    if not prompt and not args.fix:
        print("Error: No prompt provided.")
        sys.exit(1)

    # Maestro Integration
    if link:
        prompt = link.inject_maestro_context(prompt)

    if args.step and not args.yes:
        prompt = handle_step(prompt)

    if args.fix:
        if not script_path:
            print("Error: --fix requires a script file as input.")
            sys.exit(1)
        sys.exit(run_fix_loop(args, prompt, script_path))

    if args.debate:
        sys.exit(run_debate(args, prompt))

    difficulty = args.difficulty
    if args.eval_difficulty:
        difficulty = eval_difficulty(prompt, args.backend)

    model = MODELS[args.backend].get(difficulty, MODELS[args.backend].get("mid"))
    
    print(f"Executing {args.backend} using {model if model else 'Auto'} tier...", file=sys.stderr)
    
    start_time = datetime.now()
    rc, stdout, stderr = run_backend(args.backend, model, prompt, yolo=args.yolo, files=args.files)
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    history_entry = {
        "timestamp": start_time.isoformat(),
        "duration_sec": duration,
        "backend": args.backend,
        "model": model,
        "difficulty": difficulty,
        "prompt": prompt,
        "yolo": args.yolo,
        "success": rc == 0
    }

    if rc == 0:
        history_entry["response"] = stdout
        save_history(history_entry)
        
        # Maestro status update
        if link:
            state = link.get_maestro_context()
            task_id = state.get("task_id")
            if task_id:
                link.log_ai_event(task_id, {
                    "backend": args.backend,
                    "model": model,
                    "difficulty": difficulty,
                    "duration": duration,
                    "yolo": args.yolo
                })
                if args.fix:
                    link.update_task_status(task_id, "Fixed", f"Auto-fixed using {model}")
                elif args.yolo:
                    link.update_task_status(task_id, "Attempted", "Unrestricted (YOLO) execution completed")
    else:
        print(f"\nError (Exit Code {rc}):", file=sys.stderr)
        if stderr:
            print(stderr, file=sys.stderr)
        history_entry["error"] = stderr
        save_history(history_entry)
        
        # Maestro status update for failure
        if link:
            state = link.get_maestro_context()
            task_id = state.get("task_id")
            if task_id:
                link.update_task_status(task_id, "Failed", f"Execution failed with code {rc}: {stderr[:100]}...")
        
        sys.exit(rc)

if __name__ == "__main__":
    main()
