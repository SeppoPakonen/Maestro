#!/usr/bin/env python3
# Copyright: Seppo Pakonen 2026 (C)
# License: GPLv3

import sys
import argparse
import os
import subprocess
import json
import tempfile
from pathlib import Path
from datetime import datetime

# Ensure maestro package is discoverable
repo_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(repo_root))

try:
    from maestro.ai.maestro_link import get_link
    link = get_link()
except ImportError:
    link = None

# 2026 Model Specifications
MODELS = {
    "codex": {
        "hard": "gpt-5.2-codex", # Extra High Reasoning
        "mid": "gpt-5.1-codex-max",
        "easy": "gpt-5.1-codex-mini",
        "general": "gpt-5.2",
        "judge": "gpt-5.2-codex"
    },
    "claude": {
        "hard": "opus-4.5",
        "mid": "sonnet-4.5",
        "easy": "haiku-4.5"
    },
    "gemini": {
        "hard": "gemini-3-flash-preview",
        "mid": "gemini-2.5-pro",
        "easy": "gemini-2.5-flash-lite",
        "auto": None # Let CLI decide
    },
    "qwen": {
        "hard": "qwen3-coder",
        "mid": "qwen3-coder",
        "easy": "qwen3-coder",
        "coder": "qwen3-coder",
        "vision": "qwen3-vl"
    }
}

MAESTRO_DIR = Path.home() / ".maestro"
AI_RUN_DIR = MAESTRO_DIR / "ai-run"
HISTORY_FILE = AI_RUN_DIR / "history.jsonl"

def setup_directories():
    AI_RUN_DIR.mkdir(parents=True, exist_ok=True)

def save_history(entry):
    try:
        setup_directories()
        with open(HISTORY_FILE, "a") as f:
            f.write(json.dumps(entry) + "\n")
    except Exception as e:
        print(f"Warning: Failed to save history: {e}", file=sys.stderr)

def run_backend(backend, model, prompt, stream=True, yolo=False, files=None):
    cmd = []
    if backend == "gemini":
        cmd = ["gemini"]
        if yolo:
            cmd.append("-y")
        else:
            cmd.extend(["--approval-mode", "default"])
        if model:
            cmd.extend(["-m", model])
        if files:
            for f in files:
                cmd.extend(["--attach", f])
        cmd.append(prompt)
    elif backend == "claude":
        cmd = ["claude", "-p"]
        if yolo:
            cmd.append("--dangerously-skip-permissions")
        if model:
            cmd.extend(["--model", model])
        if files:
            # Note: claude CLI might handle files differently, assuming positional for now or --add-dir
            for f in files:
                cmd.extend(["--add-dir", str(Path(f).parent)])
        cmd.append(prompt)
    elif backend == "codex":
        cmd = ["codex", "exec"]
        if yolo:
            cmd.append("--dangerously-bypass-approvals-and-sandbox")
        if model:
            cmd.extend(["-m", model])
        if files:
            for f in files:
                cmd.extend(["--image", f]) # Assuming multimodal via --image for now
        cmd.append(prompt)
    elif backend == "qwen":
        cmd = ["qwen"]
        if yolo:
            cmd.append("-y")
        if model:
            cmd.extend(["-m", model])
        # Assuming qwen handles multimodal files similarly to gemini if not specified
        cmd.append(prompt)

    if yolo:
        print("WARNING: YOLO mode active. Bypassing safety restrictions and sandboxes.", file=sys.stderr)

    try:
        if stream:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)
            stdout_text = []
            
            for line in process.stdout:
                sys.stdout.write(line)
                sys.stdout.flush()
                stdout_text.append(line)
            
            _, stderr = process.communicate()
            return process.returncode, "".join(stdout_text), stderr
        else:
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            stdout, stderr = process.communicate()
            return process.returncode, stdout, stderr
    except Exception as e:
        return -1, "", str(e)

def eval_difficulty(prompt, backend="gemini"):
    eval_prompt = f"Assess the difficulty of the following task. Respond only with one word: 'easy', 'mid', or 'hard'.\n\nTask: {prompt}"
    rc, stdout, stderr = run_backend(backend, None, eval_prompt, stream=False)
    if rc == 0:
        result = stdout.strip().lower()
        if "hard" in result: return "hard"
        if "mid" in result: return "mid"
        if "easy" in result: return "easy"
    return "mid" # Fallback

def check_status():
    print("Checking backend status...")
    backends = ["gemini", "claude", "codex", "qwen"]
    for b in backends:
        try:
            subprocess.run(["which", b], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            print(f"{b:8}: Available")
        except subprocess.CalledProcessError:
            print(f"{b:8}: NOT FOUND")
    
    print("\nQuota Information (Estimated):")
    print("All backends appearing healthy. Consult provider dashboards for exact billing/quota.")

def run_gen_image(prompt, output_file, backend="gemini"):
    print(f"Generating image: '{prompt}' -> {output_file}...")
    # Integration for image generation using 2026 models
    # Assuming gemini or codex has a --gen-image or similar subcommand in 2026
    if backend == "gemini":
        cmd = ["gemini", "imagine", prompt, "--output", output_file]
    else: # codex
        cmd = ["codex", "generate-image", prompt, "--output", output_file]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"Image saved to {output_file}")
            return 0
        else:
            print(f"Error generating image: {result.stderr}", file=sys.stderr)
            return result.returncode
    except Exception as e:
        print(f"Failed to run image generation: {e}", file=sys.stderr)
        return -1

def run_fix_loop(args, prompt, script_path):
    retries = 3
    for attempt in range(retries):
        print(f"Attempting to run script: {script_path} (Attempt {attempt+1}/{retries})...")
        process = subprocess.run([sys.executable, script_path], capture_output=True, text=True)
        if process.returncode == 0:
            print("Execution successful.")
            print(process.stdout)
            return 0
        
        print(f"Execution failed with exit code {process.returncode}.", file=sys.stderr)
        print("Capturing stderr for self-healing...", file=sys.stderr)
        
        fix_prompt = f"""
The following Python script failed execution.
Exit Code: {process.returncode}
Stderr:
{process.stderr}

Script Content:
```python
{Path(script_path).read_text()}
```

Please fix the script. Respond ONLY with the corrected code inside a python code block.
"""
        model = MODELS[args.backend].get(args.difficulty, MODELS[args.backend].get("mid"))
        rc, stdout, stderr = run_backend(args.backend, model, fix_prompt, stream=False, yolo=args.yolo)
        
        if rc == 0:
            # Extract code block
            import re
            code_match = re.search(r"```python\n(.*?)```", stdout, re.DOTALL)
            if not code_match:
                code_match = re.search(r"```\n(.*?)```", stdout, re.DOTALL)
            
            if code_match:
                fixed_code = code_match.group(1)
                Path(script_path).write_text(fixed_code)
                print("Applied fix. Retrying...")
                
                # Log fix attempt to Maestro
                if link:
                    state = link.get_maestro_context()
                    task_id = state.get("task_id")
                    if task_id:
                        link.log_ai_event(task_id, {
                            "action": "fix_attempt",
                            "attempt": attempt + 1,
                            "backend": args.backend,
                            "model": model
                        })
            else:
                print("Could not extract fixed code from AI response.", file=sys.stderr)
                break
        else:
            print(f"AI failed to provide a fix: {stderr}", file=sys.stderr)
            break
            
    return 1

def run_debate(args, prompt):
    print("Entering Debate Mode (Consensus synthesize)...")
    
    # Send to two backends
    backends = [("claude", MODELS["claude"]["hard"]), ("gemini", MODELS["gemini"]["hard"])]
    responses = []
    
    for name, model in backends:
        print(f"Requesting perspective from {name}...")
        rc, stdout, stderr = run_backend(name, model, prompt, stream=False, yolo=args.yolo)
        if rc == 0:
            responses.append((name, stdout))
        else:
            print(f"Warning: {name} failed to respond: {stderr}", file=sys.stderr)

    if len(responses) < 2:
        print("Debate failed: Not enough backends responded.", file=sys.stderr)
        return 1

    # Pass to Judge (Codex)
    judge_model = MODELS["codex"]["judge"]
    judge_prompt = f"""
You are the Judge in a multi-model debate. 
Original Prompt: {prompt} 

Response from {responses[0][0]}:
{responses[0][1]}

Response from {responses[1][0]}:
{responses[1][1]}

Synthesize a final consensus response that captures the best of both perspectives.
"""
    print(f"Passing debate to Judge ({judge_model})...")
    rc, stdout, stderr = run_backend("codex", judge_model, judge_prompt, stream=True, yolo=args.yolo)
    
    if rc == 0 and link:
        state = link.get_maestro_context()
        task_id = state.get("task_id")
        if task_id:
            link.log_ai_event(task_id, {
                "action": "debate_consensus",
                "backends": [r[0] for r in responses],
                "judge": "codex"
            })
            
    return rc

def handle_step(prompt):
    with tempfile.NamedTemporaryFile(suffix=".md", mode="w+", delete=False) as tf:
        tf.write(prompt)
        temp_path = tf.name
    
    editor = os.environ.get("EDITOR", "vi")
    print(f"Pausing for human review. Opening {editor}...")
    subprocess.run([editor, temp_path])
    
    with open(temp_path, "r") as f:
        new_prompt = f.read()
    
    os.unlink(temp_path)
    return new_prompt

def main():
    parser = argparse.ArgumentParser(
        description="ai-run: A smart wrapper for AI prompts.",
        add_help=False
    )
    # Core Arguments
    parser.add_argument("-b", "--backend", choices=["gemini", "codex", "claude", "qwen"], default="gemini", help="Select provider")
    parser.add_argument("-d", "--difficulty", choices=["easy", "mid", "hard", "coder", "vision", "general"], default="mid", help="Manually set tier")
    parser.add_argument("-e", "--eval-difficulty", "-ed", action="store_true", help="Run a pre-prompt to assess task difficulty")
    parser.add_argument("-y", "--yolo", action="store_true", help="Enable unrestricted execution (bypass safety/sandboxes)")
    parser.add_argument("-Y", "--yes", action="store_true", help="Auto-approve all prompts and skip interactive pauses (implies --yolo)")
    parser.add_argument("--status", action="store_true", help="Check API availability and remaining quota")
    
    # Agentic Modes
    parser.add_argument("--fix", action="store_true", help="Self-healing mode: fix and re-run failing scripts")
    parser.add_argument("--debate", action="store_true", help="Debate mode: Consensus from multiple models")
    parser.add_argument("--step", action="store_true", help="Interactive pause: edit prompt/plan in $EDITOR before execution")
    
    # Multimodal & Generative
    parser.add_argument("--gen-image", help="Generate an image based on the prompt")
    parser.add_argument("-o", "--output", help="Output file for generated images or logs")
    
    # General
    parser.add_argument("-h", "--help", action="help", help="Show this help message and exit")
    parser.add_argument("prompt_or_file", nargs="?", help="Prompt text or file containing prompt")
    parser.add_argument("files", nargs="*", help="Additional files (images/audio/scripts)")

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(0)

    args, unknown = parser.parse_known_args()

    if args.status:
        check_status()
        sys.exit(0)

    if args.gen_image:
        sys.exit(run_gen_image(args.gen_image, args.output or "output.png", args.backend))

    prompt = ""
    script_path = None
    if args.prompt_or_file:
        if os.path.isfile(args.prompt_or_file):
            script_path = args.prompt_or_file
            with open(args.prompt_or_file, "r") as f:
                lines = f.readlines()
                if lines and lines[0].startswith("#!"):
                    start_idx = 1
                    while start_idx < len(lines) and (not lines[start_idx].strip() or lines[start_idx].strip().startswith("#")):
                        start_idx += 1
                    prompt = "".join(lines[start_idx:])
                else:
                    prompt = "".join(lines)
        else:
            prompt = args.prompt_or_file
    
    if not prompt and not sys.stdin.isatty():
        prompt = sys.stdin.read()
    
    if not prompt and not args.fix:
        print("Error: No prompt provided.")
        sys.exit(1)

    # Maestro Integration
    if link:
        prompt = link.inject_maestro_context(prompt)

    if args.step and not args.yes:
        prompt = handle_step(prompt)

    if args.fix:
        if not script_path:
            print("Error: --fix requires a script file as input.")
            sys.exit(1)
        sys.exit(run_fix_loop(args, prompt, script_path))

    if args.debate:
        sys.exit(run_debate(args, prompt))

    difficulty = args.difficulty
    if args.eval_difficulty:
        difficulty = eval_difficulty(prompt, args.backend)

    model = MODELS[args.backend].get(difficulty, MODELS[args.backend].get("mid"))
    
    print(f"Executing {args.backend} using {model if model else 'Auto'} tier...", file=sys.stderr)
    
    start_time = datetime.now()
    rc, stdout, stderr = run_backend(args.backend, model, prompt, yolo=args.yolo, files=args.files)
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    history_entry = {
        "timestamp": start_time.isoformat(),
        "duration_sec": duration,
        "backend": args.backend,
        "model": model,
        "difficulty": difficulty,
        "prompt": prompt,
        "yolo": args.yolo,
        "success": rc == 0
    }

    if rc == 0:
        history_entry["response"] = stdout
        save_history(history_entry)
        
        # Maestro status update
        if link:
            state = link.get_maestro_context()
            task_id = state.get("task_id")
            if task_id:
                link.log_ai_event(task_id, {
                    "backend": args.backend,
                    "model": model,
                    "difficulty": difficulty,
                    "duration": duration,
                    "yolo": args.yolo
                })
                if args.fix:
                    link.update_task_status(task_id, "Fixed", f"Auto-fixed using {model}")
                elif args.yolo:
                    link.update_task_status(task_id, "Attempted", "Unrestricted (YOLO) execution completed")
    else:
        print(f"\nError (Exit Code {rc}):", file=sys.stderr)
        if stderr:
            print(stderr, file=sys.stderr)
        history_entry["error"] = stderr
        save_history(history_entry)
        
        # Maestro status update for failure
        if link:
            state = link.get_maestro_context()
            task_id = state.get("task_id")
            if task_id:
                link.update_task_status(task_id, "Failed", f"Execution failed with code {rc}: {stderr[:100]}...")
        
        sys.exit(rc)

if __name__ == "__main__":
    main()
